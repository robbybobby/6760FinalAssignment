{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      3\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/__init__.py:2169\u001b[0m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;66;03m# Import most common subpackages\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2160\u001b[0m \n\u001b[1;32m   2161\u001b[0m \u001b[38;5;66;03m# needs to be before import torch.nn as nn to avoid circular dependencies\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m     enable_grad \u001b[38;5;28;01mas\u001b[39;00m enable_grad,\n\u001b[1;32m   2164\u001b[0m     inference_mode \u001b[38;5;28;01mas\u001b[39;00m inference_mode,\n\u001b[1;32m   2165\u001b[0m     no_grad \u001b[38;5;28;01mas\u001b[39;00m no_grad,\n\u001b[1;32m   2166\u001b[0m     set_grad_enabled \u001b[38;5;28;01mas\u001b[39;00m set_grad_enabled,\n\u001b[1;32m   2167\u001b[0m )\n\u001b[0;32m-> 2169\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   2170\u001b[0m     __config__ \u001b[38;5;28;01mas\u001b[39;00m __config__,\n\u001b[1;32m   2171\u001b[0m     __future__ \u001b[38;5;28;01mas\u001b[39;00m __future__,\n\u001b[1;32m   2172\u001b[0m     _awaits \u001b[38;5;28;01mas\u001b[39;00m _awaits,\n\u001b[1;32m   2173\u001b[0m     accelerator \u001b[38;5;28;01mas\u001b[39;00m accelerator,\n\u001b[1;32m   2174\u001b[0m     autograd \u001b[38;5;28;01mas\u001b[39;00m autograd,\n\u001b[1;32m   2175\u001b[0m     backends \u001b[38;5;28;01mas\u001b[39;00m backends,\n\u001b[1;32m   2176\u001b[0m     cpu \u001b[38;5;28;01mas\u001b[39;00m cpu,\n\u001b[1;32m   2177\u001b[0m     cuda \u001b[38;5;28;01mas\u001b[39;00m cuda,\n\u001b[1;32m   2178\u001b[0m     distributed \u001b[38;5;28;01mas\u001b[39;00m distributed,\n\u001b[1;32m   2179\u001b[0m     distributions \u001b[38;5;28;01mas\u001b[39;00m distributions,\n\u001b[1;32m   2180\u001b[0m     fft \u001b[38;5;28;01mas\u001b[39;00m fft,\n\u001b[1;32m   2181\u001b[0m     futures \u001b[38;5;28;01mas\u001b[39;00m futures,\n\u001b[1;32m   2182\u001b[0m     hub \u001b[38;5;28;01mas\u001b[39;00m hub,\n\u001b[1;32m   2183\u001b[0m     jit \u001b[38;5;28;01mas\u001b[39;00m jit,\n\u001b[1;32m   2184\u001b[0m     linalg \u001b[38;5;28;01mas\u001b[39;00m linalg,\n\u001b[1;32m   2185\u001b[0m     mps \u001b[38;5;28;01mas\u001b[39;00m mps,\n\u001b[1;32m   2186\u001b[0m     mtia \u001b[38;5;28;01mas\u001b[39;00m mtia,\n\u001b[1;32m   2187\u001b[0m     multiprocessing \u001b[38;5;28;01mas\u001b[39;00m multiprocessing,\n\u001b[1;32m   2188\u001b[0m     nested \u001b[38;5;28;01mas\u001b[39;00m nested,\n\u001b[1;32m   2189\u001b[0m     nn \u001b[38;5;28;01mas\u001b[39;00m nn,\n\u001b[1;32m   2190\u001b[0m     optim \u001b[38;5;28;01mas\u001b[39;00m optim,\n\u001b[1;32m   2191\u001b[0m     overrides \u001b[38;5;28;01mas\u001b[39;00m overrides,\n\u001b[1;32m   2192\u001b[0m     profiler \u001b[38;5;28;01mas\u001b[39;00m profiler,\n\u001b[1;32m   2193\u001b[0m     sparse \u001b[38;5;28;01mas\u001b[39;00m sparse,\n\u001b[1;32m   2194\u001b[0m     special \u001b[38;5;28;01mas\u001b[39;00m special,\n\u001b[1;32m   2195\u001b[0m     testing \u001b[38;5;28;01mas\u001b[39;00m testing,\n\u001b[1;32m   2196\u001b[0m     types \u001b[38;5;28;01mas\u001b[39;00m types,\n\u001b[1;32m   2197\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[1;32m   2198\u001b[0m     xpu \u001b[38;5;28;01mas\u001b[39;00m xpu,\n\u001b[1;32m   2199\u001b[0m )\n\u001b[1;32m   2200\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m windows \u001b[38;5;28;01mas\u001b[39;00m windows\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;66;03m# Quantized, sparse, AO, etc. should be last to get imported, as nothing\u001b[39;00m\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;66;03m# is expected to depend on them.\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nested/__init__.py:21\u001b[0m\n\u001b[1;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_padded_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_nested_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_select\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Allowlist these for weights_only load of NJT\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _rebuild_njt, NestedTensor \u001b[38;5;28;01mas\u001b[39;00m _NestedTensor\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39madd_safe_globals([_NestedTensor, _rebuild_njt])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mas_nested_tensor\u001b[39m(\n\u001b[1;32m     28\u001b[0m     ts: Union[Tensor, \u001b[38;5;28mlist\u001b[39m[Tensor], \u001b[38;5;28mtuple\u001b[39m[Tensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]],\n\u001b[1;32m     29\u001b[0m     dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     30\u001b[0m     device: Optional[Device] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m     layout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "BATCH_SIZE = 1\n",
    "MAX_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "forget_texts = [\"Biologie et génétique des cellules humaines.\"]\n",
    "retain_texts = [\"La révolution française commence en 1789.\"]\n",
    "print(\":rocket: Chargement du modèle Mixtral (frozen + updated)...\")\n",
    "model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "frozen_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "updated_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def forward_with_cache(model, inputs, module):\n",
    "    cache = []\n",
    "    def hook(module, input, output):\n",
    "        cache.append(output[0] if isinstance(output, tuple) else output)\n",
    "    handle = module.register_forward_hook(hook)\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    handle.remove()\n",
    "    return cache[0]\n",
    "forget_inputs = tokenizer(forget_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "forget_inputs = {k: v.to(DEVICE) for k, v in forget_inputs.items()}\n",
    "retain_inputs = tokenizer(retain_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
    "retain_inputs = {k: v.to(DEVICE) for k, v in retain_inputs.items()}\n",
    "layer_id = 7\n",
    "updated_module = updated_model.model.layers[layer_id]\n",
    "frozen_module = frozen_model.model.layers[layer_id]\n",
    "print(\":arrows_counterclockwise: Forward forget\")\n",
    "forget_acts = forward_with_cache(updated_model, forget_inputs, updated_module)\n",
    "print(f\":white_check_mark: forget shape: {forget_acts.shape} | dtype: {forget_acts.dtype}\")\n",
    "print(\":arrows_counterclockwise: Forward retain\")\n",
    "retain_acts = forward_with_cache(frozen_model, retain_inputs, frozen_module)\n",
    "print(f\":white_check_mark: retain shape: {retain_acts.shape} | dtype: {retain_acts.dtype}\")\n",
    "print(f\":brain: VRAM utilisée : {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\":brain: VRAM max : {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  21%|███▌             | 4/19 [00:38<02:28,  9.90s/it]"
     ]
    }
   ],
   "source": [
    "# best\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5\"\n",
    "\n",
    "!python3 -m rmu.unlearn_WMDP \\\n",
    "  --model_name mistralai/Mixtral-8x7B-Instruct-v0.1  \\\n",
    "  --batch_size 1 \\\n",
    "  --max_num_batches 20 \\\n",
    "  --param_ids 7 \\\n",
    "  --retain_corpora wikitext,wikitext \\\n",
    "  --forget_corpora bio-forget-corpus,cyber-forget-corpus \\\n",
    "  --steering_coeffs 300,300 \\\n",
    "  --alpha 1600,1600 \\\n",
    "  --min_len 200 \\\n",
    "  --lr 5e-5 \\\n",
    "  --seed 42 \\\n",
    "  --output_dir models/mixtral_rmu \\\n",
    "  --verbose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' !python -m rmu.unlearn   --model_name_or_path \"microsoft/DialoGPT-medium\"   --forget_corpora \"forget_qa\"   --retain_corpora \"retain_qa\"   --alpha \"30\"   --steering_coeffs \"10\"   --lr 5e-5   --max_num_batches 200   --layer_id 12   --verbose '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Other pipelines we tested\"\"\"\n",
    "\"\"\" !python3 -m rmu.unlearn \\\n",
    "  --model_name mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "  --batch_size 1 \\\n",
    "  --param_ids 7 \\\n",
    "  --max_num_batches 200 \\\n",
    "  --retain_corpora retain_qa,retain_qa_icl \\\n",
    "  --forget_corpora forget_qa,forget_qa_icl \\\n",
    "  --steering_coeffs 300 \\\n",
    "  --alpha 1600 \\\n",
    "  --min_len 200 \\\n",
    "  --lr 5e-5 \\\n",
    "  --seed 42 \\\n",
    "  --output_dir models/mixtral_muse_rmu \\\n",
    "  --verbose \"\"\"\n",
    "\"\"\" !python -m rmu.unlearn \\\n",
    "  --model_name_or_path mistralai/Mixtral-8x7B-Instruct-v0.1 \\\n",
    "  --batch_size 1 \\\n",
    "  --param_ids 7 \\\n",
    "  --max_num_batches 200 \\\n",
    "  --retain_corpora retain_qa,retain_qa_icl \\\n",
    "  --forget_corpora forget_qa,forget_qa_icl \\\n",
    "  --steering_coeffs 300,300 \\\n",
    "  --alpha 1600,1600 \\\n",
    "  --min_len 200 \\\n",
    "  --lr 5e-5 \\\n",
    "  --seed 42 \\\n",
    "  --output_dir models/mixtral_muse_rmu \\\n",
    "  --verbose \"\"\"\n",
    "\"\"\" !python -m rmu.unlearn \\\n",
    "  --model_name_or_path \"microsoft/DialoGPT-medium\" \\\n",
    "  --forget_corpora \"forget_qa\" \\\n",
    "  --retain_corpora \"retain_qa\" \\\n",
    "  --alpha \"30\" \\\n",
    "  --steering_coeffs \"10\" \\\n",
    "  --lr 5e-5 \\\n",
    "  --max_num_batches 200 \\\n",
    "  --layer_id 12 \\\n",
    "  --verbose \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-30:23:41:54 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2025-06-30:23:41:54 INFO     [__main__:428] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`\n",
      "2025-06-30:23:41:54 INFO     [__main__:440] Selected Tasks: ['mmlu', 'wmdp']\n",
      "2025-06-30:23:41:54 WARNING  [evaluator:163] Model appears to be an instruct variant but chat template is not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-06-30:23:41:54 INFO     [evaluator:189] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-06-30:23:41:54 INFO     [evaluator:227] Initializing hf model, with arguments: {'pretrained': 'tiiuae/falcon-7b-instruct', 'load_in_8bit': True, 'token': True, 'device': 'cuda', 'trust_remote_code': True}\n",
      "2025-06-30:23:41:54 INFO     [models.huggingface:137] Using device 'cuda'\n",
      "2025-06-30:23:41:54 WARNING  [transformers_modules.tiiuae.falcon-7b-instruct.8782b5c5d8c9290412416618f36a133653e85285.configuration_falcon:328] \n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n",
      "2025-06-30:23:41:55 INFO     [models.huggingface:382] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "2025-06-30:23:41:56 WARNING  [bitsandbytes.cextension:282] The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'cuda', 'hpu', 'mps', 'xpu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'npu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/bin/lm-eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "  File \"/teamspace/studios/this_studio/lm-evaluation-harness/lm_eval/__main__.py\", line 449, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "  File \"/teamspace/studios/this_studio/lm-evaluation-harness/lm_eval/utils.py\", line 439, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/teamspace/studios/this_studio/lm-evaluation-harness/lm_eval/evaluator.py\", line 230, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "  File \"/teamspace/studios/this_studio/lm-evaluation-harness/lm_eval/api/model.py\", line 151, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "  File \"/teamspace/studios/this_studio/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 193, in __init__\n",
      "    self._create_model(\n",
      "  File \"/teamspace/studios/this_studio/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 594, in _create_model\n",
      "    self._model = self.AUTO_MODEL_CLASS.from_pretrained(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 309, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4390, in from_pretrained\n",
      "    hf_quantizer.validate_environment(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_8bit.py\", line 81, in validate_environment\n",
      "    validate_bnb_backend_availability(raise_exception=True)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py\", line 560, in validate_bnb_backend_availability\n",
      "    return _validate_bnb_multi_backend_availability(raise_exception)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/integrations/bitsandbytes.py\", line 517, in _validate_bnb_multi_backend_availability\n",
      "    raise RuntimeError(err_msg)\n",
      "RuntimeError: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'cuda', 'hpu', 'mps', 'xpu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'npu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' !lm-eval   --model hf   --model_args \"pretrained=models/mixtral_muse_rmu,load_in_4bit=true,bnb_4bit_compute_dtype=float16,bnb_4bit_use_double_quant=true,bnb_4bit_quant_type=nf4,low_cpu_mem_usage=true,device_map=auto,trust_remote_code=true\"   --batch_size 1   --tasks muse_knowmem   --limit 1000 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" !lm-eval --model hf \\\n",
    "    --model_args pretrained=models/mixtral_rmu,parallelize=True \\\n",
    "    --tasks wmdp,mmlu \\\n",
    "    --batch_size=32 \"\"\"\n",
    "\"\"\"!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=mistralai/Mixtral-8x7B-Instruct-v0.1,use_auth_token=True \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/mixtral_results.json\"\"\"\n",
    "\"\"\"!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=EleutherAI/gpt-neo-1.3B,dtype=float16,device=cuda \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/gptneo_results.json\"\"\"\n",
    "\"\"\"!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=tiiuae/falcon-7b-instruct,dtype=float16,device=cuda \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/falcon_results.json\"\"\"\n",
    "\"\"\"!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=mistralai/Mixtral-8x7B-Instruct-v0.1,use_auth_token=True,dtype=float16 \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/mixtral_results.json\"\"\"\n",
    "\"\"\" !lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=mistralai/Mixtral-8x7B-Instruct-v0.1,use_auth_token=True,device=cpu \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/mixtral_results.json \"\"\"\n",
    "\"\"\"!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=mistralai/Mixtral-8x7B-Instruct-v0.1,use_auth_token=True,device=cpu \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/mixtral_results.json \\\n",
    "  --trust_remote_code\"\"\"\n",
    "\"\"\"!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=tiiuae/falcon-7b-instruct,dtype=float16,device=cuda \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/falcon7b_results.json \\\n",
    "  --trust_remote_code\"\"\"\n",
    "\"\"\" !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True && \\\n",
    "lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=tiiuae/falcon-7b-instruct,token=True,dtype=float16,device=cuda \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/falcon7b_results.json \\\n",
    "  --trust_remote_code   \"\"\"\n",
    "\"\"\"!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=tiiuae/falcon-7b-instruct,token=True,dtype=float16,device=cuda \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/falcon7b_results.json \\\n",
    "  --trust_remote_code \\\n",
    "  --batch_size 1\"\"\"\n",
    "# Working for WMDP\n",
    "!lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=tiiuae/falcon-7b-instruct,load_in_8bit=True,token=True,device=cuda \\\n",
    "  --tasks mmlu,wmdp \\\n",
    "  --output_path results/eval_results.json \\\n",
    "  --trust_remote_code \\\n",
    "  --batch_size 1 \\\n",
    "  --limit 1000\n",
    "\n",
    "\"\"\" !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True && \\\n",
    "lm-eval \\\n",
    "  --model hf-causal-experimental \\\n",
    "  --model_args pretrained=./models/mixtral_muse_rmu,trust_remote_code=True \\\n",
    "  --tasks muse_knowmem,muse_privleak \\\n",
    "  --output_path results/mixtral_muse_eval.json \\\n",
    "  --trust_remote_code \\\n",
    "  --batch_size 1 \\\n",
    "  --limit 1000 \"\"\"\n",
    "#!lm-eval --tasks list | grep muse\n",
    "\"\"\" !lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args pretrained=models/mixtral_muse_rmu \\\n",
    "  --tasks muse_knowmem,muse_privleak \\\n",
    "  --num_fewshot 0 \\\n",
    "  --batch_size 1 \\\n",
    "  --output_path results/mixtral_muse_rmu_eval.json \\\n",
    "  --limit 1000 \"\"\"\n",
    "\"\"\" !lm-eval --model hf \\\n",
    "  --model_args pretrained=models/mixtral_muse_rmu \\\n",
    "  --tasks muse_knowmem \\\n",
    "  --device cuda \\\n",
    "  --batch_size 1 \"\"\"\n",
    "\"\"\" !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True && \\\n",
    "lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args \"pretrained=models/mixtral_muse_rmu,load_in_4bit=true,bnb_4bit_compute_dtype=float16,device_map=auto,trust_remote_code=true\" \\\n",
    "  --batch_size 1 \\\n",
    "  --tasks muse_knowmem \\\n",
    "  --limit 1000 \"\"\"\n",
    "\"\"\" !lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args \"pretrained=models/mixtral_muse_rmu,load_in_4bit=true,bnb_4bit_compute_dtype=float16,bnb_4bit_use_double_quant=true,device_map=auto,trust_remote_code=true\" \\\n",
    "  --batch_size 1 \\\n",
    "  --tasks muse_knowmem \\\n",
    "  --limit 1000 \"\"\"\n",
    "\"\"\" !lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args \"pretrained=models/mixtral_muse_rmu,load_in_4bit=true,bnb_4bit_compute_dtype=float16,bnb_4bit_use_double_quant=true,bnb_4bit_quant_type=nf4,low_cpu_mem_usage=true,device_map=auto,trust_remote_code=true\" \\\n",
    "  --batch_size 1 \\\n",
    "  --tasks muse_knowmem \\\n",
    "  --limit 1000 \"\"\"\n",
    "\"\"\" !lm-eval \\\n",
    "  --model hf \\\n",
    "  --model_args \"pretrained=models/mixtral_muse_rmu,load_in_4bit=true,bnb_4bit_compute_dtype=float16,bnb_4bit_use_double_quant=true,bnb_4bit_quant_type=nf4,low_cpu_mem_usage=true,device_map=auto,trust_remote_code=true\" \\\n",
    "  --batch_size 1 \\\n",
    "  --tasks muse_knowmem \\\n",
    "  --limit 1000 \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
